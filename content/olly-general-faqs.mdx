---
title: "FAQs"
---

Find answers to the most frequently asked questions about Olly.

## Product positioning & architecture


<Accordion title="What is the difference between Olly and Coralogix AI Center?">
    Olly is a standalone offering of AI across all Coralogix’s capabilities - logs, metrics traces. The AI Center, on the other hand, is a place to monitor and track AI applications.
</Accordion>

<Accordion title="What is the difference between Olly and Coralogix MCP?">
    MCP is access to Coralogix data. Olly is intelligence. While MCP lets AI tools read observability data, Olly uses that data to actually run production investigations.
    
    **What Coralogix MCP is**:

    - MCP is a way to expose Coralogix API to AI systems like Cursor.
    - Coralogix MCP exposes logs, metrics, traces, alerts.
    - It’s an integration layer, not a product.

    **What Olly is**:

    Olly sits on top of the Coralogix Observability Platform and turns data into action. It is an **Agentic Observability System** made of:

        - **Knowledge**: Understands your services, history, incidents, deployments - like an engineer who knows your production.
            
        - **Multi-agents**: Specialized agents (logs, traces, metrics, correlation, security, engineering) that work together on any task and prompt.
            
        - **Autonomy**: Autonomously decide on how to navigate and solve any challenge.
            
        - **UX & UI**: Olly’s UI is specially built for Observability use-cases - support any insight with evidence, generate relevant charts and recommend actions - built for humans in production.
        
    Olly behaves like a Senior Production Engineer and investigates, reasons, and takes action, not just queries data.
</Accordion>

<Accordion title="Why Olly is a separate product and doesn’t reside within Coralogix UI?">
    Olly is separate because it delivers something observability platforms can’t: **a clean, AI-native experience** focused entirely on reliability outcomes. By keeping it independent, we remove the noise and complexity of traditional observability tools - giving you faster detection, clearer answers, and much lower MTTR with almost zero setup.

    By building Olly separately, customers get three huge advantages:

    1. You get answers immediately - not dashboards to configure.
    2. No setup, no tuning, no maintenance.
    3. Much faster time-to-value.
</Accordion>

## Integration & platform capabilities

<Accordion title="Does Olly have an API? How does Olly integrate with Coralogix?">
    Olly does not currently offer a public API.
    Olly is natively integrated with Coralogix. There’s no separate backend service or data onboarding process required:

        - Customers don’t need to ingest data into Olly.
        - Olly works directly on top of existing Coralogix data and infrastructure.
</Accordion>


<Accordion title="Is RUM data supported in Olly??">
    RUM sessions are not visible to Olly yet.
    However, Olly **does have access to RUM logs**, which means it can still analyze RUM-related data and provide insights for many RUM-focused questions.
    Support for full RUM session visibility is not available at the moment, and there’s no confirmed timeline yet — but this is an area we’re actively aware of.
    If your use case relies on analyzing RUM logs (rather than sessions), Olly should already be able to help.
</Accordion>

## Token usage & limits

<Accordion title="Can I monitor my token usage?">
    Yes - You can monitor your token usage by navigating to your profile, then **Usage**.
    Usage is tracked per your and resets monthly.
</Accordion>

<Accordion title="Can a few questions consume a large number of Olly tokens?">
    This can absolutely happen.

    Olly’s token usage is not based on the number of questions, but on the total amount of data and context processed to answer them.

    Even a small number of questions can consume a high number of tokens if they require deeper analysis. Each request may include:

        - **Conversation context**: Olly sends the full relevant conversation history to keep answers accurate and consistent.
        - **Retrieved supporting data**: Olly may pull in large volumes of logs, traces, metrics, alerts, or other data to properly analyze your question.
        - **Model reasoning and output**: Tokens are used not only for the final answer, but also for intermediate reasoning and processing steps.

    Because of this, a question that takes longer to analyze or touches a broader dataset can consume significantly more tokens than a simple question — even if you only asked a few.
</Accordion>

<Accordion title="How are tokens calculated in Olly?">
    Olly follows the same token model used by LLMs such as OpenAI and Claude.
    When you ask a question, Olly may run multiple agents behind the scenes to understand the request, retrieve relevant data, and generate an accurate answer. Because of this, more complex questions consume more tokens.
    As a rule of thumb, a token roughly represents a word. To keep things simple and transparent, Olly displays input and output tokens together as a single “Token” unit in the UI.
</Accordion>

<Accordion title="Will I be notified when I reach my token limit?">
    When a user reaches their token limit (based on their seat tier), Olly blocks further usage. This state is clearly shown in the UI
</Accordion>